{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpenglow.evaluation import DcgScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datawand.parametrization import ParamHelper\n",
    "ph = ParamHelper('..', 'LinkPrediction', sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.read_csv(\"/mnt/idms/fberes/data/bitcoin_ln_research/link_prediction/data/links_df_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20#None#30#ph.get(\"top_first_days\")\n",
    "top_k = 500#100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K == None:\n",
    "    delta_t = 86400*7\n",
    "else:\n",
    "    delta_t = 86400#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"onmf_dim10_lr0.140_nr100\",\n",
    "    #\"bomf_dim10_lr0.140_nr100\",\n",
    "    #\"offmf_dim10_lr0.050_nr100\",\n",
    "    \"pop\",\n",
    "    \"time_pop\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_dir = \"/mnt/idms/fberes/data/bitcoin_ln_research/link_prediction/rankings/topk%i_exkTrue_%s/\" % (top_k,str(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = [pd.read_csv(\"%s/%s.csv\" % (ranking_dir,m)) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(df) for df in rankings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings[0]['time'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = 1548982800 # (GMT): Friday, February 1, 2019 1:00:00 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeframe(df, delta_t, min_time=1548982800):\n",
    "    df[\"timeframe\"] = df[\"time\"].apply(lambda x: max(0,(x-min_time)//delta_t))\n",
    "\n",
    "for i in range(len(rankings)):\n",
    "    get_timeframe(rankings[i], delta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.) average performance (online DCG)\n",
    "\n",
    "The average performance for the offline batch model is confusing (it is only bad on the first day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mean_dcg(with_first_day=True):\n",
    "    if with_first_day:\n",
    "        mean_dcgs = [df[\"dcg\"].mean() for df in rankings]\n",
    "        df = rankings[0]\n",
    "        print(len(df))\n",
    "    else:\n",
    "        mean_dcgs = [df[df[\"timeframe\"]>0][\"dcg\"].mean() for df in rankings]\n",
    "        df = rankings[0]\n",
    "        print(len(df[df[\"timeframe\"]>0]))\n",
    "    return pd.DataFrame(list(zip(models, mean_dcgs)), columns=[\"model\",\"dcg\"]).sort_values(\"dcg\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global mean performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mean_dcg(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean performance without first day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mean_dcg(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclude known: False**\n",
    "0 \tonline \t0.139660\n",
    "1 \tbatch+online \t0.131745\n",
    "2 \tpop+time \t0.124183\n",
    "3 \tpop \t0.077110\n",
    "4 \tbatch \t0.064587\n",
    "\n",
    "**Exclude known: True - Mi\u00e9rt teljesen uaz?**\n",
    "0 \tonline \t0.139660\n",
    "1 \tbatch+online \t0.131745\n",
    "2 \tpop+time \t0.124183\n",
    "3 \tpop \t0.077110\n",
    "4 \tbatch \t0.064587"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Performance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, ranking in enumerate(rankings):\n",
    "    averages = ranking.groupby(\"timeframe\")[\"dcg\"].mean()\n",
    "    plt.plot(averages, label=models[idx])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Number of records over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = rankings[0].groupby(\"timeframe\")[\"dcg\"].count()\n",
    "plt.plot(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_sim_cols = [\"opt_income\",\"global_traffic\",\"global_income\",\"inbound_depletions\",\"high_degree\",\"high_cap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_link_sim_experiment(model_dir):\n",
    "    model_id = model_dir.split(\"/\")[-2]\n",
    "    print(model_id)\n",
    "    model_files = os.listdir(model_dir)\n",
    "    chunks = [pd.read_csv(\"%s/%s\" % (model_dir, f)) for f in model_files if \"snapshot\" in f]\n",
    "    concatenated = pd.concat(chunks)\n",
    "    print(len(model_files), len(concatenated))\n",
    "    get_timeframe(concatenated, delta_t)\n",
    "    for rank_col in link_sim_cols:\n",
    "        print(rank_col)\n",
    "        concatenated[\"dcg_\"+rank_col] = DcgScore(concatenated.rename({rank_col:\"rank\"}, axis=1))\n",
    "        print(concatenated[\"dcg_\"+rank_col].mean())\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"50000sat_k6000_aNone_e0.05_dropTrue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_results = [load_link_sim_experiment(\"%s/%s/\" % (ranking_dir, experiment_id)) for experiment_id in experiments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all simulation experiments into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_preds = simulation_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(experiments[1:]):\n",
    "    print(model)\n",
    "    sim_preds = sim_preds.merge(simulation_results[idx+1][[\"record_id\",model]], on=\"record_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining baselines with simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    sim_preds = sim_preds.merge(rankings[idx][[\"id\",\"dcg\"]].rename({\"id\":\"record_id\",\"dcg\":model}, axis=1), on=\"record_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sim_preds[experiments+models].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_preds[models+ [\"dcg_opt_income\",\"dcg_global_traffic\",\"dcg_global_income\",\"dcg_inbound_depletions\",\"dcg_high_degree\",\"dcg_high_cap\"]].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onmf_preds = pd.read_csv(\"%s/preds_onmf_dim10_lr0.140_nr100.csv\" % ranking_dir)\n",
    "onmf_preds = onmf_preds.drop(\"dcg\", axis=1)\n",
    "onmf_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpop_preds = pd.read_csv(\"%s/preds_time_pop.csv\" % ranking_dir)\n",
    "tpop_preds = tpop_preds.drop(\"dcg\", axis=1)\n",
    "tpop_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_model_preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_model = experiments[0]\n",
    "for m in link_sim_cols:\n",
    "    tmp_parts = [pd.read_csv(\"%s/%s/preds_%s_%i.csv\" % (ranking_dir,sim_model,m,i)) for i in range(19)]\n",
    "    tmp = pd.concat(tmp_parts, ignore_index=True)\n",
    "    sim_model_preds[m] = tmp\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter for common records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_record_ids = set(onmf_preds[\"record_id\"])\n",
    "len(common_record_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_record_ids = set(sim_model_preds[\"opt_income\"][\"record_id\"])\n",
    "len(common_record_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in link_sim_cols:\n",
    "    tmp = sim_model_preds[m]\n",
    "    sim_model_preds[m] = tmp[tmp[\"record_id\"].isin(common_record_ids)]\n",
    "    print(m, sim_model_preds[m].shape)\n",
    "onmf_preds = onmf_preds[onmf_preds[\"record_id\"].isin(common_record_ids)]\n",
    "print(\"onmf\", onmf_preds.shape)\n",
    "tpop_preds = tpop_preds[tpop_preds[\"record_id\"].isin(common_record_ids)]\n",
    "print(\"tpop\", tpop_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract real targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_targets = dict(links_df.loc[common_record_ids][\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(real_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recoding keys to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_2_id = dict(zip(links_df[\"src\"],links_df[\"user\"]))\n",
    "node_2_id.update(dict(zip(links_df[\"trg\"],links_df[\"item\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in link_sim_cols:\n",
    "    tmp = sim_model_preds[m]\n",
    "    tmp[\"user\"] = tmp[\"user\"].apply(lambda x: node_2_id[x])\n",
    "    tmp[\"item\"] = tmp[\"item\"].apply(lambda x: node_2_id[x])\n",
    "    sim_model_preds[m] = tmp\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_model_preds[\"onmf\"] = onmf_preds\n",
    "sim_model_preds[\"tpop\"] = tpop_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "import concurrent.futures\n",
    "\n",
    "def performance(pred_df,real_targets):\n",
    "    \"\"\"real_targets contains true items for only the related sessions\"\"\"\n",
    "    df = pred_df.copy()\n",
    "    df.loc[:,\"true_item\"] = df[\"record_id\"].apply(lambda x: real_targets[x])\n",
    "    hits = df[df[\"true_item\"]==df[\"item\"]]\n",
    "    hits.loc[:,\"dcg\"] = 1.0 / (np.log2(hits[\"rank\"]+1.0))\n",
    "    return hits[\"rec_rank\"].sum() / len(real_targets), hits[\"dcg\"].sum() / len(real_targets)\n",
    "\n",
    "def extract_topk(df, key_col, score_col, k):\n",
    "    groups = dict(tuple(df.groupby(key_col)))\n",
    "    topk_parts = [groups[key].nlargest(k,score_col) for key in groups]\n",
    "    return pd.concat(topk_parts, ignore_index=True)\n",
    "\n",
    "def combine_ranks(preds_1, preds_2, k, alpha, trim=True):\n",
    "    cols = [\"record_id\",\"item\",\"rec_rank\"]\n",
    "    p1 = preds_1[cols].copy()\n",
    "    p2 = preds_2[cols].copy()\n",
    "    p1.loc[:,\"rec_rank\"] = p1[\"rec_rank\"]*alpha\n",
    "    p2.loc[:,\"rec_rank\"] = p2[\"rec_rank\"]*(1.0-alpha)\n",
    "    combined = pd.concat([p1,p2], ignore_index=True)\n",
    "    combined = combined.groupby([\"record_id\",\"item\"])[\"rec_rank\"].sum().reset_index()\n",
    "    combined = combined[combined[\"rec_rank\"]>0]\n",
    "    combined.loc[:,\"rank\"] = 1.0 / combined[\"rec_rank\"]\n",
    "    combined.loc[:,\"dcg\"] = DcgScore(combined)\n",
    "    if trim:\n",
    "        old_size = len(combined)\n",
    "        combined = extract_topk(combined, \"record_id\", \"rec_rank\", k)\n",
    "        print(alpha,len(p1),len(combined),old_size/len(p1))\n",
    "    return combined\n",
    "\n",
    "def combine_and_eval(model1, model2, true_targets, k, a):\n",
    "    combi = combine_ranks(model1, model2, k, a)\n",
    "    mrr, mdcg = performance(combi, true_targets)\n",
    "    #print(len(combi)/len(model2))\n",
    "    return mrr, mdcg\n",
    "\n",
    "def combine_models(model1, model2, true_targets, alphas, filter_m1=False, max_threads=4):\n",
    "    if filter_m1:\n",
    "        m1 = model1[model1[\"prediction\"]>0]\n",
    "        print(\"filter\",len(model1),len(m1))\n",
    "    else:\n",
    "        m1 = model1\n",
    "    f_partial = functools.partial(combine_and_eval, m1, model2, true_targets, top_k)\n",
    "    executor = concurrent.futures.ProcessPoolExecutor(max_threads)\n",
    "    res = list(executor.map(f_partial, alphas))\n",
    "    executor.shutdown()\n",
    "    mrrs, mdcgs = zip(*res)\n",
    "    #print(mrrs)\n",
    "    print(mdcgs)\n",
    "    #plt.figure()\n",
    "    #plt.plot(alphas,mrrs,'bx',label=\"mrr\")\n",
    "    #plt.plot(alphas,mdcgs,'rx',label=\"mdcg\")\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in sim_model_preds:\n",
    "    tmp = sim_model_preds[col]\n",
    "    tmp[\"rec_rank\"] = 1.0 / tmp[\"rank\"]\n",
    "    sim_model_preds[col] = tmp\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### double check: single model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in sim_model_preds:\n",
    "    print(col, performance(sim_model_preds[col], real_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas = np.arange(0,1.1,0.1)# 0.075 is best: 0.239715\n",
    "alphas = [0.0,0.001,0.002,0.005,0.01,0.02,0.03,0.1,1.0]\n",
    "#alphas = [0.0,0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.5,1.0]\n",
    "#alphas = [0.0,0.025,0.05,0.075,0.1,0.15,0.2,1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: finally for topk=500 models can combine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_models(tpop_preds, onmf_preds, real_targets, alphas, max_threads=len(alphas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for col in link_sim_cols:\n",
    "    print(\"###%s###\" % col)\n",
    "    combine_models(sim_model_preds[col], onmf_preds, real_targets, alphas, filter_m1=False, max_threads=len(alphas))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in link_sim_cols:\n",
    "    print(\"###%s###\" % col)\n",
    "    combine_models(sim_model_preds[col], onmf_preds, real_targets, alphas, filter_m1=True, max_threads=len(alphas))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine_models(only_sim_preds, onmf_preds, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betweeness baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from link_pred_simulator import load_graph_snapshots, process_links_for_simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snapshots, time_boundaries = load_graph_snapshots(\"/mnt/idms/fberes/data/bitcoin_ln_research/directed_graphs/directed_temporal_multi_edges_1days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links_for_sim = process_links_for_simulator(links_df, None, time_boundaries, only_eval=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links_for_sim[\"eval\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_rank(scores_df, true_target, top_k, seen_nodes=None):\n",
    "        # scores_df is preordered\n",
    "        if seen_nodes == None:\n",
    "            ordered_list = list(scores_df[\"index\"])[:top_k]\n",
    "        else:\n",
    "            ordered_list = list(scores_df[~scores_df[\"index\"].isin(seen_nodes)][\"index\"])[:top_k]\n",
    "        return ordered_list.index(true_target)+1.0 if true_target in ordered_list else None\n",
    "\n",
    "def extract_central_nodes(file_path, metric):\n",
    "    scores_df = pd.read_csv(file_path, usecols=[\"index\",\"betw\"])\n",
    "    return scores_df.sort_values(metric, ascending=False)[[\"index\",metric]]\n",
    "    \n",
    "class BetweenessModel():\n",
    "    def __init__(self, centrality_dir=\"/mnt/idms/fberes/data/bitcoin_ln_research/centrality_scores\", weight=None):\n",
    "        self.metric = \"betw\"\n",
    "        self.weight = weight\n",
    "        self.centrality_dir = centrality_dir\n",
    "        self.top_k_preds = {}\n",
    "        \n",
    "    def run(self, links, k, exclude_known=True):\n",
    "        self.graph = {}\n",
    "        max_snap_id = links[\"snapshot\"].max()\n",
    "        for snap_id in range(max_snap_id+1):\n",
    "            f_path = \"%s/scores_%s_%i.csv\" % (self.centrality_dir, self.weight, snap_id)\n",
    "            self.top_k_preds[snap_id] = extract_central_nodes(f_path, self.metric)\n",
    "        ranks = []\n",
    "        indices = links.index\n",
    "        for idx in tqdm(indices):\n",
    "            row = links.loc[idx]\n",
    "            snap_id, src, trg, eval_ = row[\"snapshot\"], row[\"src\"], row[\"trg\"], row[\"eval\"]\n",
    "            if not src in self.graph:\n",
    "                self.graph[src] = set()\n",
    "            if eval_:\n",
    "                seen_nodes = self.graph[src] if exclude_known else None\n",
    "                ranks.append(calculate_rank(self.top_k_preds[snap_id], trg, k, seen_nodes))\n",
    "            else:\n",
    "                ranks.append(None)\n",
    "            self.graph[src].add(trg)\n",
    "        rankings = links.copy()\n",
    "        rankings[\"rank\"] = ranks\n",
    "        return rankings[rankings[\"eval\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can it be the same DCG???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm = BetweenessModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings = bm.run(links_for_sim, 20, exclude_known=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings[\"dcg\"] = DcgScore(bm_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings[\"dcg\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings_f = bm.run(links_for_sim, 20, exclude_known=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings[\"dcg\"] = DcgScore(bm_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings[\"dcg\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(\"/mnt/idms/fberes/data/bitcoin_ln_research/link_prediction/data/channel_events_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = pd.read_csv(\"../DynamicNetworkAnalysis/ln.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(\"562210x2014x0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2[0].apply(lambdabda x: long(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dm-3-env] *",
   "language": "python",
   "name": "conda-env-dm-3-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}