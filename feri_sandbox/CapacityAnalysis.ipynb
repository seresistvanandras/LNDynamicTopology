{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, kendalltau, weightedtau\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ln_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_directed = True\n",
    "time_window = 86400*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../LNdata/lncaptures/lngraph/2019/\"\n",
    "graph_files +=  [data_dir + f for f in sorted(os.listdir(data_dir)) if \".json\" in f]\n",
    "MIN_TIME = 1549065601-86400 #Saturday, February 2, 2019 12:00:01 AM\n",
    "#MAX_TIME = 1552867201 #Monday, March 18, 2019 12:00:01 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../LNdata/\"\n",
    "#graph_files = [data_dir + f for f in sorted(os.listdir(data_dir)) if \".json\" in f]\n",
    "graph_files += [data_dir + f for f in sorted(os.listdir(data_dir)) if \".json\" in f][5:]\n",
    "#MIN_TIME = 1552478399 # Wednesday, March 13, 2019 11:59:59 AM\n",
    "MAX_TIME = 1553947199 # Saturday, March 30, 2019 11:59:59 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = None#20#10\n",
    "if K != None:\n",
    "    graph_files = graph_files[:K]\n",
    "#graph_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_KEYS = [\"node1_pub\",\"node2_pub\",\"last_update\",\"capacity\",\"channel_id\",'node1_policy','node2_policy']\n",
    "nodes, edges = load_temp_data(graph_files[:-1], edge_keys=EDGE_KEYS)\n",
    "print(len(nodes), len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nodes[(nodes[\"last_update\"] > MIN_TIME) & (nodes[\"last_update\"] < MAX_TIME)]\n",
    "edges = edges[(edges[\"last_update\"] > MIN_TIME) & (edges[\"last_update\"] < MAX_TIME)]\n",
    "len(nodes), len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = edges.sort_values(\"last_update\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.iloc[0][\"node1_policy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract homophily and new channels\n",
    "\n",
    "- time of an edge channel is the 'last_update' timestamp\n",
    "- we suppose: first occurrence of a channel is the creation time -> **first last_update value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-filtering for duplicated channels ids\n",
    "\n",
    "- we filter for the first occurrence of each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(edges))\n",
    "edges = edges.drop_duplicates(subset=\"channel_id\", keep=\"first\")\n",
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def discover_changes(edge_updates_df):\n",
    "    edge_updates_df[\"capacity\"] = edge_updates_df[\"capacity\"].astype(\"float64\")\n",
    "    channel_state = {}\n",
    "    channel_nodes = {}\n",
    "    channel_events = []\n",
    "    policy_events = []\n",
    "    seen_nodes, seen_edges = set(), set()\n",
    "    indices = edge_updates_df.index\n",
    "    for idx in tqdm(indices, mininterval=10):\n",
    "        row = edge_updates_df.loc[idx]\n",
    "        # channel events\n",
    "        n1p, n2p, chan_id, last_update, cap = row[\"node1_pub\"], row[\"node2_pub\"], row[\"channel_id\"], row[\"last_update\"], row[\"capacity\"]\n",
    "        is_new_channel = chan_id not in channel_state\n",
    "        if (n1p,n2p) in seen_edges or (n2p,n1p) in seen_edges:\n",
    "            is_new_edge = False\n",
    "        else:\n",
    "            is_new_edge = True\n",
    "            seen_edges.add((n1p,n2p))\n",
    "        if n1p in seen_nodes and n2p in seen_nodes:\n",
    "            is_homophily = True\n",
    "        else:\n",
    "            is_homophily = False\n",
    "            seen_nodes.add(n1p)\n",
    "            seen_nodes.add(n2p)\n",
    "        cap_change = 0\n",
    "        if not is_new_channel:\n",
    "            cap_change = cap - channel_state[chan_id]\n",
    "        else:\n",
    "            channel_nodes[chan_id] = (n1p,n2p)\n",
    "        channel_state[chan_id] = cap\n",
    "        channel_events.append([last_update, chan_id, is_new_channel, is_new_edge, is_homophily, cap, cap_change])\n",
    "        # policy events\n",
    "        n1_pol, n2_pol = row[\"node1_policy\"], row[\"node2_policy\"]\n",
    "        if n1_pol != None:\n",
    "            n1_pol[\"node\"] = n1p\n",
    "            n1_pol[\"channel_id\"] = chan_id\n",
    "            n1_pol[\"new_channel\"] = is_new_channel\n",
    "            n1_pol[\"time\"] = last_update\n",
    "            policy_events.append(n1_pol)\n",
    "        if n2_pol != None:\n",
    "            n2_pol[\"node\"] = n2p\n",
    "            n2_pol[\"channel_id\"] = chan_id\n",
    "            n2_pol[\"new_channel\"] = is_new_channel\n",
    "            n2_pol[\"time\"] = last_update\n",
    "            policy_events.append(n2_pol)\n",
    "    channel_events_df = pd.DataFrame(channel_events, columns=[\"time\",\"channel_id\",\"new_channel\",\"new_edge\",\"homophily\",\"capacity\",\"cap_diff\"])\n",
    "    return channel_events_df, channel_nodes, pd.DataFrame(policy_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events, channel_nodes, policy_events_df = discover_changes(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "_ = G.add_edges_from(list(channel_nodes.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[\"new_channel\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[\"new_edge\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[\"homophily\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.to_csv(\"/mnt/idms/fberes/data/bitcoin_ln_research/directed_graphs/channel_events_%s.csv\" % str(K), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for homophily edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_channels = events[events[\"new_channel\"] & events[\"new_edge\"] & events[\"homophily\"]]\n",
    "new_channels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = new_channels[\"time\"].min()\n",
    "split = start + 86400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate link prediction with bi-directional edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: this step will boost online and time-decayed based methods!!! We should train these models 2-step wise!!!???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_channels[\"rnd\"] = np.random.random(size=len(new_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pred_edges = []\n",
    "for idx, row in new_channels.iterrows():\n",
    "    t = row[\"time\"]\n",
    "    n1, n2 = channel_nodes[row[\"channel_id\"]]\n",
    "    if row[\"rnd\"] < 0.5:\n",
    "        link_pred_edges.append((n1,n2,t,1))\n",
    "        link_pred_edges.append((n2,n1,t,0))\n",
    "    else:\n",
    "        link_pred_edges.append((n2,n1,t,1))\n",
    "        link_pred_edges.append((n1,n2,t,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.DataFrame(link_pred_edges, columns=[\"user\",\"item\",\"time\",\"eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set(links_df[\"user\"]).union(set(links_df[\"item\"]))\n",
    "recoder = dict(zip(nodes,range(len(nodes))))\n",
    "links_df[\"user\"] = links_df[\"user\"].apply(lambda x: recoder[x])\n",
    "links_df[\"item\"] = links_df[\"item\"].apply(lambda x: recoder[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i.) Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20#100\n",
    "seed = 254938879\n",
    "dim = 10\n",
    "neg_rate = 100\n",
    "ex_known = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpenglow.experiments import FactorExperiment, BatchFactorExperiment, BatchAndOnlineFactorExperiment, PopularityExperiment, PopularityTimeframeExperiment\n",
    "from alpenglow.evaluation import DcgScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_model_experiment = FactorExperiment(\n",
    "    top_k=k,\n",
    "    seed=seed,\n",
    "    dimension=dim,\n",
    "    learning_rate=0.14,\n",
    "    negative_rate=neg_rate\n",
    ")\n",
    "\n",
    "on_rankings = factor_model_experiment.run(links_df, exclude_known=ex_known, verbose=True)\n",
    "on_rankings['dcg'] = DcgScore(on_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_model_experiment = BatchFactorExperiment(\n",
    "    top_k=k,\n",
    "    seed=seed,\n",
    "    dimension=dim,\n",
    "    learning_rate=0.05,\n",
    "    negative_rate=neg_rate\n",
    ")\n",
    "\n",
    "off_rankings = batch_model_experiment.run(links_df, exclude_known=ex_known, verbose=True)\n",
    "off_rankings['dcg'] = DcgScore(off_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onoff_model_experiment = BatchAndOnlineFactorExperiment(\n",
    "    top_k=k,\n",
    "    seed=seed,\n",
    "    dimension=dim,\n",
    "    learning_rate=0.14,\n",
    "    negative_rate=neg_rate\n",
    ")\n",
    "\n",
    "onoff_rankings = onoff_model_experiment.run(links_df, exclude_known=ex_known, verbose=True)\n",
    "onoff_rankings['dcg'] = DcgScore(onoff_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_experiment = PopularityExperiment(\n",
    "    top_k=k,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "pop_rankings = pop_experiment.run(links_df, exclude_known=ex_known, verbose=True)\n",
    "pop_rankings['dcg'] = DcgScore(pop_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_t_experiment = PopularityTimeframeExperiment(\n",
    "    top_k=k,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "pop_t_rankings = pop_t_experiment.run(links_df, exclude_known=ex_known, verbose=True)\n",
    "pop_t_rankings['dcg'] = DcgScore(pop_t_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii.) Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"online\",\"batch\",\"batch+online\",\"pop\",\"pop+time\"]\n",
    "rankings = [on_rankings, off_rankings, onoff_rankings, pop_rankings, pop_t_rankings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.) average performance (online DCG)\n",
    "\n",
    "The average performance for the offline batch model is confusing (it is only bad on the first day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df[\"dcg\"].mean() for df in rankings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(df) for df in rankings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Performance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 86400*7\n",
    "\n",
    "for idx, rankings in enumerate(rankings):\n",
    "    averages = rankings['dcg'].groupby((rankings['time']-rankings['time'].min())//day).mean()\n",
    "    plt.plot(averages, label=labels[idx])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Number of records over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = on_rankings['dcg'].groupby((on_rankings['time']-on_rankings['time'].min())//day).count()\n",
    "plt.plot(cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dm-3-env] *",
   "language": "python",
   "name": "conda-env-dm-3-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}