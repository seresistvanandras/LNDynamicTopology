{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, kendalltau, weightedtau\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ln_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_directed = True\n",
    "time_window = 86400*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../LNdata/lncaptures/lngraph/2019/\"\n",
    "graph_files +=  [data_dir + f for f in sorted(os.listdir(data_dir)) if \".json\" in f]\n",
    "MIN_TIME = 1549065601-86400 #Saturday, February 2, 2019 12:00:01 AM\n",
    "#MAX_TIME = 1552867201 #Monday, March 18, 2019 12:00:01 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../LNdata/\"\n",
    "#graph_files = [data_dir + f for f in sorted(os.listdir(data_dir)) if \".json\" in f]\n",
    "graph_files += [data_dir + f for f in sorted(os.listdir(data_dir)) if \".json\" in f][5:]\n",
    "#MIN_TIME = 1552478399 # Wednesday, March 13, 2019 11:59:59 AM\n",
    "MAX_TIME = 1553947199 # Saturday, March 30, 2019 11:59:59 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_files = graph_files[:10]\n",
    "#graph_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_KEYS = [\"node1_pub\",\"node2_pub\",\"last_update\",\"capacity\",\"channel_id\",'node1_policy','node2_policy']\n",
    "nodes, edges = load_temp_data(graph_files[:-1], edge_keys=EDGE_KEYS)\n",
    "print(len(nodes), len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nodes[(nodes[\"last_update\"] > MIN_TIME) & (nodes[\"last_update\"] < MAX_TIME)]\n",
    "edges = edges[(edges[\"last_update\"] > MIN_TIME) & (edges[\"last_update\"] < MAX_TIME)]\n",
    "len(nodes), len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = edges.sort_values(\"last_update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.iloc[0][\"node1_policy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discover_changes(edge_updates_df):\n",
    "    edge_updates_df[\"capacity\"] = edge_updates_df[\"capacity\"].astype(\"float64\")\n",
    "    channel_state = {}\n",
    "    channel_nodes = {}\n",
    "    channel_events = []\n",
    "    policy_events = []\n",
    "    for idx, row in edge_updates_df.iterrows():\n",
    "        # channel events\n",
    "        n1p, n2p, chan_id, last_update, cap = row[\"node1_pub\"], row[\"node2_pub\"], row[\"channel_id\"], row[\"last_update\"], row[\"capacity\"]\n",
    "        is_new_channel = chan_id not in channel_state\n",
    "        cap_change = 0\n",
    "        if not is_new_channel:\n",
    "            cap_change = cap - channel_state[chan_id]\n",
    "        else:\n",
    "            channel_nodes[chan_id] = (n1p,n2p)\n",
    "        channel_state[chan_id] = cap\n",
    "        channel_events.append([last_update, chan_id, is_new_channel, cap, cap_change])\n",
    "        # policy events\n",
    "        n1_pol, n2_pol = row[\"node1_policy\"], row[\"node2_policy\"]\n",
    "        if n1_pol != None:\n",
    "            n1_pol[\"node\"] = n1p\n",
    "            n1_pol[\"channel_id\"] = chan_id\n",
    "            n1_pol[\"new_channel\"] = is_new_channel\n",
    "            n1_pol[\"time\"] = last_update\n",
    "            policy_events.append(n1_pol)\n",
    "        if n2_pol != None:\n",
    "            n2_pol[\"node\"] = n2p\n",
    "            n2_pol[\"channel_id\"] = chan_id\n",
    "            n2_pol[\"new_channel\"] = is_new_channel\n",
    "            n2_pol[\"time\"] = last_update\n",
    "            policy_events.append(n2_pol)\n",
    "    channel_events_df = pd.DataFrame(channel_events, columns=[\"time\",\"channel_id\",\"is_new\",\"capacity\",\"cap_diff\"])\n",
    "    return channel_events_df, channel_nodes, pd.DataFrame(policy_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events, channel_nodes, policy_events_df = discover_changes(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "_ = G.add_edges_from(list(channel_nodes.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degs = dict(G.degree())\n",
    "pr = nx.pagerank(G)\n",
    "betw = nx.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy_events_df = policy_events_df[~policy_events_df[\"disabled\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy_events_df.shape)\n",
    "policy_events_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of times the policy was changed for a channel node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only_pol_updates = policy_events_df\n",
    "only_pol_updates = policy_events_df[~policy_events_df[\"new_channel\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(policy_events_df), len(only_pol_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"fee_base_msat\":1000,\"fee_rate_milli_msat\":1,\"min_htlc\":1000}\n",
    "pricing_cols = [\"fee_base_msat\",\"fee_rate_milli_msat\",\"min_htlc\"]\n",
    "for col in pricing_cols:\n",
    "    policy_events_df[col] = policy_events_df[col].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_mean_pricing = only_pol_updates.groupby(\"node\")[pricing_cols].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node_mean_pricing[\"min_htlc\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_chan_changes = only_pol_updates.groupby([\"node\",\"channel_id\"])[\"time\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_chan_changes[\"time\"].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_stats = node_chan_changes.groupby(\"node\")[\"time\"].mean().reset_index()\n",
    "node_stats.columns = [\"node\",\"mean_num_policy_changes\"]\n",
    "node_stats[\"degree\"] = node_stats[\"node\"].apply(lambda x: degs.get(x,0.0))\n",
    "node_stats[\"pr\"] = node_stats[\"node\"].apply(lambda x: pr.get(x,0.0))\n",
    "node_stats[\"betw\"] = node_stats[\"node\"].apply(lambda x: betw.get(x,0.0))\n",
    "node_stats = node_stats.merge(node_mean_pricing, on=\"node\", how=\"left\")\n",
    "node_stats = node_stats.set_index(\"node\")\n",
    "node_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_stats.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=node_stats, x=\"mean_num_policy_changes\", y=\"degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[\"is_new\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(events[\"cap_diff\"] != 0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "events.to_csv(\"channel_events.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: predict the change in channel capacity!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import alpenglow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_channels = events[events[\"is_new\"]]\n",
    "new_channels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = new_channels[\"time\"].min()\n",
    "split = start + 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_pred_edges = []\n",
    "for idx, row in new_channels.iterrows():\n",
    "    t = row[\"time\"]\n",
    "    n1, n2 = channel_nodes[row[\"channel_id\"]]\n",
    "    link_pred_edges.append((n1,n2,t))\n",
    "    link_pred_edges.append((n2,n1,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links_df = pd.DataFrame(link_pred_edges, columns=[\"user\",\"item\",\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes = set(links_df[\"user\"]).union(set(links_df[\"item\"]))\n",
    "recoder = dict(zip(nodes,range(len(nodes))))\n",
    "links_df[\"user\"] = links_df[\"user\"].apply(lambda x: recoder[x])\n",
    "links_df[\"item\"] = links_df[\"item\"].apply(lambda x: recoder[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpenglow.experiments import FactorExperiment\n",
    "from alpenglow.evaluation import DcgScore\n",
    "\n",
    "factor_model_experiment = FactorExperiment(\n",
    "    top_k=100,\n",
    "    seed=254938879,\n",
    "    dimension=10,\n",
    "    learning_rate=0.14,\n",
    "    negative_rate=100\n",
    ")\n",
    "\n",
    "rankings = factor_model_experiment.run(links_df, exclude_known=False, verbose=True)\n",
    "rankings['dcg'] = DcgScore(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a bit high! no surprise.. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings['dcg'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 86400\n",
    "averages = rankings['dcg'].groupby((rankings['time']-rankings['time'].min())//day).mean()\n",
    "plt.plot(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = rankings['dcg'].groupby((rankings['time']-rankings['time'].min())//day).count()\n",
    "plt.plot(cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}