{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpenglow.evaluation import DcgScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datawand.parametrization import ParamHelper\n",
    "ph = ParamHelper('..', 'LinkPrediction', sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.read_csv(\"/mnt/idms/fberes/data/bitcoin_ln_research/link_prediction/data/links_df_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20#None#30#ph.get(\"top_first_days\")\n",
    "top_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K == None:\n",
    "    delta_t = 86400*7\n",
    "else:\n",
    "    delta_t = 86400#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"onmf_dim10_lr0.140_nr100\",\n",
    "    \"bomf_dim10_lr0.140_nr100\",\n",
    "    \"offmf_dim10_lr0.050_nr100\",\n",
    "    \"pop\",\n",
    "    \"time_pop\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_dir = \"/mnt/idms/fberes/data/bitcoin_ln_research/link_prediction/rankings/topk%i_exkTrue_%s/\" % (top_k,str(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = [pd.read_csv(\"%s/%s.csv\" % (ranking_dir,m)) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(df) for df in rankings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings[0]['time'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = 1548982800 # (GMT): Friday, February 1, 2019 1:00:00 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeframe(df, delta_t, min_time=1548982800):\n",
    "    df[\"timeframe\"] = df[\"time\"].apply(lambda x: max(0,(x-min_time)//delta_t))\n",
    "\n",
    "for i in range(len(rankings)):\n",
    "    get_timeframe(rankings[i], delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings[4].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.) average performance (online DCG)\n",
    "\n",
    "The average performance for the offline batch model is confusing (it is only bad on the first day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mean_dcg(with_first_day=True):\n",
    "    if with_first_day:\n",
    "        mean_dcgs = [df[\"dcg\"].mean() for df in rankings]\n",
    "        df = rankings[0]\n",
    "        print(len(df))\n",
    "    else:\n",
    "        mean_dcgs = [df[df[\"timeframe\"]>0][\"dcg\"].mean() for df in rankings]\n",
    "        df = rankings[0]\n",
    "        print(len(df[df[\"timeframe\"]>0]))\n",
    "    return pd.DataFrame(list(zip(models, mean_dcgs)), columns=[\"model\",\"dcg\"]).sort_values(\"dcg\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global mean performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mean_dcg(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean performance without first day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mean_dcg(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclude known: False**\n",
    "0 \tonline \t0.139660\n",
    "1 \tbatch+online \t0.131745\n",
    "2 \tpop+time \t0.124183\n",
    "3 \tpop \t0.077110\n",
    "4 \tbatch \t0.064587\n",
    "\n",
    "**Exclude known: True - Mi\u00e9rt teljesen uaz?**\n",
    "0 \tonline \t0.139660\n",
    "1 \tbatch+online \t0.131745\n",
    "2 \tpop+time \t0.124183\n",
    "3 \tpop \t0.077110\n",
    "4 \tbatch \t0.064587"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Performance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, ranking in enumerate(rankings):\n",
    "    averages = ranking.groupby(\"timeframe\")[\"dcg\"].mean()\n",
    "    plt.plot(averages, label=models[idx])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Number of records over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = rankings[0].groupby(\"timeframe\")[\"dcg\"].count()\n",
    "plt.plot(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation based results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment_id = \"200000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_link_sim_experiment(model_dir):\n",
    "    model_id = model_dir.split(\"/\")[-2]\n",
    "    print(model_id)\n",
    "    model_files = os.listdir(model_dir)\n",
    "    chunks = [pd.read_csv(\"%s/%s\" % (model_dir, f)) for f in model_files if \"snapshot\" in f]\n",
    "    concatenated = pd.concat(chunks)\n",
    "    print(len(model_files), len(concatenated))\n",
    "    get_timeframe(concatenated, delta_t)\n",
    "    #print(concatenated.isnull().sum() / len(concatenated))\n",
    "    #print(concatenated.head())\n",
    "    for rank_col in [\"rank1\",\"rank2\",\"rank3\",\"rank4\"]:\n",
    "        print(rank_col)\n",
    "        concatenated[\"dcg_\"+rank_col] = DcgScore(concatenated.rename({rank_col:\"rank\"}, axis=1))\n",
    "        print(concatenated[rank_col].mean())\n",
    "    return concatenated\n",
    "    #concatenated[model_id] = DcgScore(concatenated)\n",
    "    #print(concatenated[model_id].mean())\n",
    "    #concatenated['base_dcg'] = DcgScore(concatenated.drop(\"rank\",axis=1).rename({\"base\":\"rank\"}, axis=1))\n",
    "    #print(concatenated[\"base_dcg\"].mean())\n",
    "    #return concatenated.drop(\"base_dcg\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiments = [\n",
    "    \"200000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "    \"200000sat_k10000_aNone_e0.05_dropFalse-onmf_dim10_lr0.140_nr100\",\n",
    "    \"200000sat_k10000_a2.0_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiments = [\n",
    "\"200000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "#\"100000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "#\"200000sat_k10000_a2.0_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "#\"200000sat_k10000_aNone_e0.05_dropFalse-onmf_dim10_lr0.140_nr100\",\n",
    "#\"200000sat_k20000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "#\"200000sat_k5000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "#\"300000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "\"500000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "\"200000sat_k10000_aNone_e0.05_dropTrue-time_pop\",\n",
    "\"500000sat_k10000_aNone_e0.05_dropTrue-time_pop\",\n",
    "\"sp_200000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "\"sp_200000sat_k10000_aNone_e0.05_dropTrue-time_pop\",\n",
    "\"sp_500000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "\"sp_500000sat_k10000_aNone_e0.05_dropTrue-time_pop\",\n",
    "\"slink_200000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "\"slink_200000sat_k10000_aNone_e0.05_dropTrue-time_pop\",\n",
    "\"slink_500000sat_k10000_aNone_e0.05_dropTrue-onmf_dim10_lr0.140_nr100\",\n",
    "\"slink_500000sat_k10000_aNone_e0.05_dropTrue-time_pop\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"50000sat_k6000_aNone_e0.05_dropTrue-time_pop\"]#[\"trial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_results = [load_link_sim_experiment(\"%s/%s/\" % (ranking_dir, experiment_id)) for experiment_id in experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_preds = simulation_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(experiments[1:]):\n",
    "    sim_preds = sim_preds.merge(simulation_results[idx+1][[\"record_id\",model]], on=\"record_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining baselines with simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    sim_preds = sim_preds.merge(rankings[idx][[\"id\",\"dcg\"]].rename({\"id\":\"record_id\",\"dcg\":model}, axis=1), on=\"record_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sim_preds[experiments+models].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_preds[models+[\"dcg_rank1\",\"dcg_rank2\",\"dcg_rank3\",\"dcg_rank4\"]].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onmf_preds = pd.read_csv(\"%s/preds_onmf_dim10_lr0.140_nr100.csv\" % ranking_dir)\n",
    "onmf_preds = onmf_preds.drop(\"dcg\", axis=1)\n",
    "onmf_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpop_preds = pd.read_csv(\"%s/preds_time_pop.csv\" % ranking_dir)\n",
    "tpop_preds = tpop_preds.drop(\"dcg\", axis=1)\n",
    "tpop_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_model = experiments[0]\n",
    "sim_preds_parts = [pd.read_csv(\"%s/%s/preds_%i.csv\" % (ranking_dir,sim_model,i)) for i in range(19)]\n",
    "sim_preds = pd.concat(sim_preds_parts, ignore_index=True)\n",
    "sim_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: r\u00e1j\u00f6nni, hogy mi okozza a degree based baseline visszaes\u00e9s\u00e9t! Elvileg hasonl\u00f3 cs\u00facsokat j\u00f3sol a szimul\u00e1ci\u00f3 is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: csak a tiszta predikci\u00f3kat k\u00e9ne kombin\u00e1lni (ui. ritka event az amikor bev\u00e9tel szerint optimaliz\u00e1lna valaki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node_names = pd.read_csv(\"/mnt/idms/fberes/data/bitcoin_ln_research/node_names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only_sim_preds = sim_preds[sim_preds[\"prediction\"]>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only_cap_preds = sim_preds[sim_preds[\"prediction\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def show_avg_rank(df):\n",
    "    item_counts = df[\"item\"].value_counts()\n",
    "    most_pop = list(item_counts[item_counts > 100].index)\n",
    "    item_avg_ranks = df[df[\"item\"].isin(most_pop)].groupby(\"item\")[\"rank\"].mean().sort_values().reset_index()\n",
    "    return item_avg_ranks.merge(node_names[[\"pub_key\",\"name\",\"cap_ratio\"]], left_on=\"item\", right_on=\"pub_key\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only_sim_ranks = show_avg_rank(only_sim_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only_cap_ranks = show_avg_rank(only_cap_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only_sim_ranks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only_cap_ranks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter for common records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_record_ids = set(sim_preds[\"record_id\"])\n",
    "len(common_record_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onmf_preds = onmf_preds[onmf_preds[\"record_id\"].isin(common_record_ids)]\n",
    "onmf_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpop_preds = tpop_preds[tpop_preds[\"record_id\"].isin(common_record_ids)]\n",
    "tpop_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract real targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_targets = dict(links_df.loc[common_record_ids][\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(real_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recoding keys to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_2_id = dict(zip(links_df[\"src\"],links_df[\"user\"]))\n",
    "node_2_id.update(dict(zip(links_df[\"trg\"],links_df[\"item\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_preds[\"user\"] = sim_preds[\"user\"].apply(lambda x: node_2_id[x])\n",
    "sim_preds[\"item\"] = sim_preds[\"item\"].apply(lambda x: node_2_id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def performance(pred_df,real_targets):\n",
    "    \"\"\"real_targets contains true items for only the related sessions\"\"\"\n",
    "    df = pred_df.copy()\n",
    "    df[\"true_item\"] = df[\"record_id\"].apply(lambda x: real_targets[x])\n",
    "    hits = df[df[\"true_item\"]==df[\"item\"]]\n",
    "    hits[\"dcg\"] = 1.0 / (np.log2(hits[\"rank\"]+1.0))\n",
    "    return hits[\"rec_rank\"].sum() / len(real_targets), hits[\"dcg\"].sum() / len(real_targets)\n",
    "\n",
    "def combine_ranks(preds_1, preds_2, alpha, k):\n",
    "    cols = [\"record_id\",\"item\",\"rec_rank\"]\n",
    "    p1 = preds_1[cols].copy()\n",
    "    p2 = preds_2[cols].copy()\n",
    "    p1[\"rec_rank\"] = p1[\"rec_rank\"]*alpha\n",
    "    p2[\"rec_rank\"] = p2[\"rec_rank\"]*(1.0-alpha)\n",
    "    combined = pd.concat([p1,p2], ignore_index=True)\n",
    "    combined = combined.groupby([\"record_id\",\"item\"])[\"rec_rank\"].sum().reset_index()\n",
    "    combined = combined[combined[\"rec_rank\"]>0]\n",
    "    combined[\"rank\"] = 1.0 / combined[\"rec_rank\"]\n",
    "    combined[\"dcg\"] = DcgScore(combined)\n",
    "    # TODO: filter for top_k recommendation for each event???\n",
    "    return combined\n",
    "\n",
    "def combine_models(model1, model2, alphas):\n",
    "    mrrs, mdcgs = [], []\n",
    "    #for a in tqdm(alphas):\n",
    "    for a in alphas:\n",
    "        combi = combine_ranks(model1, model2, a, top_k)\n",
    "        mrr, mdcg = performance(combi, real_targets)\n",
    "        mrrs.append(mrr)\n",
    "        mdcgs.append(mdcg)\n",
    "        print(len(combi)/len(model2))\n",
    "    print(mrrs)\n",
    "    print(mdcgs)\n",
    "    #plt.plot(alphas,mrrs,'bx',label=\"mrr\")\n",
    "    plt.plot(alphas,mdcgs,'rx',label=\"mdcg\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onmf_preds[\"rec_rank\"] = 1.0 / onmf_preds[\"rank\"]\n",
    "sim_preds[\"rec_rank\"] = 1.0 / sim_preds[\"rank\"]\n",
    "tpop_preds[\"rec_rank\"] = 1.0 / tpop_preds[\"rank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_sim_preds = sim_preds[sim_preds[\"prediction\"]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(only_sim_preds), len(sim_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance(onmf_preds, real_targets))\n",
    "print(performance(tpop_preds, real_targets))\n",
    "print(performance(sim_preds, real_targets))\n",
    "print(performance(only_sim_preds, real_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas = np.arange(0,1.1,0.1)\n",
    "#alphas = [0.0,0.025,0.05,0.075,0.1,0.2,0.3,0.5,1.0]\n",
    "alphas = [0.0,0.01,0.02,0.03,0.04,0.05,0.1,0.5,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_models(tpop_preds, onmf_preds, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_models(sim_preds, onmf_preds, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_models(only_sim_preds, onmf_preds, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betweeness baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from link_pred_simulator import load_graph_snapshots, process_links_for_simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snapshots, time_boundaries = load_graph_snapshots(\"/mnt/idms/fberes/data/bitcoin_ln_research/directed_graphs/directed_temporal_multi_edges_1days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links_for_sim = process_links_for_simulator(links_df, None, time_boundaries, only_eval=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links_for_sim[\"eval\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_rank(scores_df, true_target, top_k, seen_nodes=None):\n",
    "        # scores_df is preordered\n",
    "        if seen_nodes == None:\n",
    "            ordered_list = list(scores_df[\"index\"])[:top_k]\n",
    "        else:\n",
    "            ordered_list = list(scores_df[~scores_df[\"index\"].isin(seen_nodes)][\"index\"])[:top_k]\n",
    "        return ordered_list.index(true_target)+1.0 if true_target in ordered_list else None\n",
    "\n",
    "def extract_central_nodes(file_path, metric):\n",
    "    scores_df = pd.read_csv(file_path, usecols=[\"index\",\"betw\"])\n",
    "    return scores_df.sort_values(metric, ascending=False)[[\"index\",metric]]\n",
    "    \n",
    "class BetweenessModel():\n",
    "    def __init__(self, centrality_dir=\"/mnt/idms/fberes/data/bitcoin_ln_research/centrality_scores\", weight=None):\n",
    "        self.metric = \"betw\"\n",
    "        self.weight = weight\n",
    "        self.centrality_dir = centrality_dir\n",
    "        self.top_k_preds = {}\n",
    "        \n",
    "    def run(self, links, k, exclude_known=True):\n",
    "        self.graph = {}\n",
    "        max_snap_id = links[\"snapshot\"].max()\n",
    "        for snap_id in range(max_snap_id+1):\n",
    "            f_path = \"%s/scores_%s_%i.csv\" % (self.centrality_dir, self.weight, snap_id)\n",
    "            self.top_k_preds[snap_id] = extract_central_nodes(f_path, self.metric)\n",
    "        ranks = []\n",
    "        indices = links.index\n",
    "        for idx in tqdm(indices):\n",
    "            row = links.loc[idx]\n",
    "            snap_id, src, trg, eval_ = row[\"snapshot\"], row[\"src\"], row[\"trg\"], row[\"eval\"]\n",
    "            if not src in self.graph:\n",
    "                self.graph[src] = set()\n",
    "            if eval_:\n",
    "                seen_nodes = self.graph[src] if exclude_known else None\n",
    "                ranks.append(calculate_rank(self.top_k_preds[snap_id], trg, k, seen_nodes))\n",
    "            else:\n",
    "                ranks.append(None)\n",
    "            self.graph[src].add(trg)\n",
    "        rankings = links.copy()\n",
    "        rankings[\"rank\"] = ranks\n",
    "        return rankings[rankings[\"eval\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can it be the same DCG???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm = BetweenessModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings = bm.run(links_for_sim, 20, exclude_known=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings[\"dcg\"] = DcgScore(bm_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings[\"dcg\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings_f = bm.run(links_for_sim, 20, exclude_known=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings[\"dcg\"] = DcgScore(bm_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bm_rankings[\"dcg\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dm-3-env] *",
   "language": "python",
   "name": "conda-env-dm-3-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}